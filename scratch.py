import re
import math
import os
#from nltk.stem import PorterStemmer
# /**
#  * Contains code to compute all features for input-based evaluation except cosine overlap (CorpusBasedUtilities.java).
#  * @author Annie Louis
#  */
class EvalFeatures:
  # /**
  #  * Computes the Jensen Shannon divergence between 2 given vocabulary distributions.
  #  * No smoothing is done in this version.
  #  */
  def getJSDivergence(self, distA, distB):
    probA, probB, jsd = 0,0,0
    complete = []
    complete.extend(distA.vocabWords)
    for c in range(len(distB.vocabWords)):
      if (not distB.vocabWords[c] in complete):
        complete.append(distB.vocabWords[c])
    for i in range(len(complete)):
      try:
        indA = distA.vocabWords.index(complete[i])
        probA = distA.vocabFreq[indA] / (distA.numTokens)
      except ValueError:
        probA = 0
      try:
        indB = distB.vocabWords.index(complete[i])
        probB = distA.vocabFreq[indB] / (distB.numTokens)
      except ValueError:
        probB = 0
      if (probA == 0):
        part1 = 0
      else:
        part1 = probA * math.log(probA / ((probA / 2) + (probB / 2)))
      if (probB == 0):
        part2 = 0
      else:
        part2 = probB * math.log(probB / ((probB / 2) + (probA / 2)))
      jsd += (part1 + part2) / 2
    return jsd
  # /**
  #  * Computes the Jensen Shannon divergence between 2 given vocabulary distributions.
  #  * Lidstone smoothing is done in this version.
  #  * Takes 2 more arguments related to smoothing:
  #  * <br> gamma - the real number estimate to be added to the count for each vocabulary item (default 0.005)
  #  * <br> bins - number of sample values that can be generated by the experiment behind the probability distribution (1.5 * vocabulary size of input)
  #  */
  def getSmoothedJSDivergence(self, distA, distB, gamma, bins):
    countA, probA, countB, probB, jsd = 0.0,0,0,0,0
    complete = []
    complete.extend(distA.vocabWords)
    for c in range(len(distB.vocabWords)):
        if (not distB.vocabWords[c] in complete):
            complete.append(distB.vocabWords[c])
    for i in range(len(complete)):
        try:
            indA = distA.vocabWords.index(complete[i])
            countA = distA.vocabFreq[indA] + gamma
        except ValueError:
            countA = gamma
        probA = countA / ((distA.numTokens) + (gamma * bins))
        try:
            indB = distB.vocabWords.index(complete[i])
            countB = distB.vocabFreq[indB] + gamma
        except ValueError:
            countB = gamma
        probB = countB / ((distB.numTokens) + (gamma * bins))
        if (probA == 0):
            part1 = 0
        else:
            part1 = probA * math.log(probA / ((probA / 2) + (probB / 2)))
        if (probB == 0):
            part2 = 0
        else:
            part2 = probB * math.log(probB / ((probB / 2) + (probA / 2)))
        jsd += (part1 + part2) / 2
    return jsd
  # /**
  #  * Computes the Kullback Leibler divergence between 2 given vocabulary distributions.
  #  * Lidstone smoothing is done in this version. Takes 2 more arguments related to smoothing.
  #  * <br> gamma - the real number estimate to be added to the count for each vocabulary item
  #  * <br> bins - number of sample values that can be generated by the experiment behind the probability distribution
  #  * <br><br> KL divergence is not symmetric. Hence the function returns an array of two values--the first is KL(A||B) and the second KL(B||A) where
  #  * A and B are two vocabulary distributions provided.
  #  */
  def getKLdivergenceSmoothed(self, distA, distB, gamma, bins):
    divDistADistB = 0
    divDistBDistA = 0
    complete = []
    complete.extend(distA.vocabWords)
    for c in range(len(distB.vocabWords)):
      if (not distB.vocabWords[c] in complete):
        complete.append(distB.vocabWords[c])
    for i in range(len(complete)):
      try:
          indA = distA.vocabWords.index(complete[i])
          countA = distA.vocabFreq[indA] + gamma
      except ValueError:
          countA = gamma
      try:
          indB = distB.vocabWords.index(complete[i])
          countB = distB.vocabFreq[indB] + gamma
      except ValueError:
          countB = gamma
      probA = countA / ((distA.numTokens) + (gamma * bins))
      probB = countB / ((distB.numTokens) + (gamma * bins))
      probA_B = probA / probB
      probB_A = probB / probA
      divDistADistB += probA * math.log(probA_B)
      divDistBDistA += probB * math.log(probB_A)
    if ((divDistADistB < 0) or (divDistBDistA < 0)):
      print(" negative div = ", divDistADistB, ",", divDistBDistA)
    KLdiv = [0,0]
    KLdiv[0] = divDistADistB
    KLdiv[1] = divDistBDistA
    return KLdiv
  # /**
  #  * Given a list of topic words and a vocabulary distribution, this function computes the percentage of tokens
  #  * from the vocabulary distribution that are topic words.
  #  */
  def getPercentTokensThatIsSignTerms(self, topicWordList, dist):
    count = 0
    for i in range(len(topicWordList)):
      signTerm = topicWordList[i]
      try:
        present = dist.vocabWords.index(signTerm)
        count += dist.vocabFreq[present]
      except:
          continue
    if (count == 0):
      return 0
    percentTokens = count / dist.numTokens
    return (percentTokens)
  # /**
  #  * Given a list of topic words and a vocabulary distribution, this function computes what fraction of topic
  #  * words in the list are also present in the given vocabulary distribution.
  #  */
  def getPercentTopicWordsCoveredByGivenDist(self, topicWordList, dist):
    count = 0
    for i in range(len(topicWordList)):
      signTerm = topicWordList[i]
      try:
        present = dist.vocabWords.index(signTerm)
        count=count+1
      except:
          continue
    if (count == 0):
      return 0
    percentCovered = count / len(topicWordList)
    return percentCovered
  # /**
  #  * This function takes two vocabulary distributions. The emission probabilities for each word is computed from the
  #  * emission distribution. The unigram likelihood of a new distribution (newDist) under this set of emission probabilities
  #  * is returned.
  #  */
  def getUnigramProbability(self, emissionDist, newDist):
    probOfNewDist = 0.0
    for i in range(len(newDist.vocabWords)):
      word = newDist.vocabWords[i]
      try:
        emitIndex = emissionDist.vocabWords.index(word)
        wordFreqInEmissionDist = emissionDist.vocabFreq[emitIndex]
      except:
        continue
      wordEmissionProbability = wordFreqInEmissionDist / emissionDist.numTokens
      wordFreqInNewDist = newDist.vocabFreq[i]
      probOfNewDist += wordFreqInNewDist * math.log10(wordEmissionProbability)
    return probOfNewDist
  # /**
  #  * This function takes two vocabulary distributions. The emission probabilities for each word is computed from the
  #  * emission distribution. The multinomial likelihood of a new distribution (new Dist) under this set of emission probabilities
  #  * is returned.
  #  */
  def getMultinomialProbability(self, emissionDist, newDist):
    unigramProb = self.getUnigramProbability(emissionDist, newDist)
    denomMultCoeff = 0
    numMultCoeff = 0
    for i in range(len(newDist.vocabWords)):
      wordFreqInNewDist = newDist.vocabFreq[i]
      fact = 1
      for j in range(1,wordFreqInNewDist):
        fact *= j
      denomMultCoeff += math.log10(fact)
    for j in range(1,newDist.numTokens):
      numMultCoeff += math.log10(j)
    multinomialCoeff = numMultCoeff - denomMultCoeff
    return (multinomialCoeff + unigramProb)
# /**
#  * Stores a set of words and their frequency counts. All text is converted into such
#  * vocabulary distributions before computing any of the features.
#  * @author Annie Louis
#  */
class vocabDist:
  def __init__(self, words, freq, tokens):
    self.vocabWords = words
    self.vocabFreq = freq
    self.numTokens = tokens
  def printStats(self):
    print("vocabulary size = " , len(self.vocabWords))
    print("total tokens = " + self.numTokens)
# /**
#  * Class where all processing involving auxiliary data is processed--cosine, computing topic signatures,
#  * storing stopword lists. Contains methods for computing vocabulary distributions and reading already
#  * created distributions into memory.
#  * @author Annie Louis
#  */
class CorpusBasedUtilities:
    # /**
    #  * Reads in any info files provided
    #  * <br> 1. stopword list
    #  * <br> 2. file with frequency counts of words from a large background corpus (to use for computing topic signatures)
    #  * <br><br> If stopwords must not be filtered, then the string must be empty "". Similarly for other files if they need not be used.
    #  */
    # /**
    #  * Read the stopwords from file into a list
    #  */
    def readStopWords(self, filepath):
        if (not os.path.exists(filepath)):
            print("Stopword file not found: " + filepath)
        br = open(filepath, 'r')
        line = br.readline()
        while (line != ""):
            if (line.strip() == ""):
                continue
            self.stopWords.append(line.strip().lower())
            line = br.readline()
        br.close()
    # /**
    #  * Read the idf values from given file
    #  */
    def readBackgroundIdf(self, filepath):
        if (not os.path.exists(filepath)):
            print("Idf file not found: " + filepath)
        br = open(filepath, 'r')
        self.totalDocs = int(br.readline().strip())
        line = br.readline()
        while (line != ""):
            if (line.strip() == ""):
                continue
            toks = line.strip().split(" ")
            wd = toks[0]
            idf = float(toks[1])
            self.wordToIdf.update({wd:idf})
            line = br.readline()
        br.close()
    # /**
    # * Read the background corpus frequency counts from given file
    # */
    def readBackgroundCorpusCounts(self, filepath, stem):
        if (not os.path.exists(filepath)):
            print("Idf file not found: " + filepath)
        br = open(filepath, 'r')
        totalToks = 0
        words = []
        freqs = []
        line = br.readline()
        while (line != ""):
            if (line.strip() == ""):
                continue
            toks = line.strip().split(" ")
            wd = toks[0]
            if (stem):
                wd = PorterStemmer().stem(wd)
            count = int(toks[1])
            totalToks += count
            # //update frequency if word already in list else add as new
            try:
                indInVocab = words.index(wd)
                cur = freqs[indInVocab]
                freqs[indInVocab] = cur + count
            except:
                words.append(wd)
                freqs.append(count)
            line = br.readline()
        # //create aggregated vocabulary dist. Only one entry per stem with frequencies totalled over all words adding to the same stem.
        global backgroundDist
        backgroundDist = vocabDist(words, freqs, totalToks)
        br.close()
    def __init__(self, conf):
        self.stopWords = []  # //this list will be empty throughout if stopword=N option is specified
        self.wordToIdf = {}
        self.totalDocs = 0
        self.stemOption = conf.performStemming # //if true, then all vocabulary distributions--input, summary, background and idf values file are stemmed.
        # //print("stemopt= "+stemOption)
        if (conf.removeStopWords):
          print("Reading stopwords from " + conf.stopFile)
          self.readStopWords(conf.stopFile)
        if (conf.topic):
          print("Reading background corpus frequency counts " + conf.bgCountFile)
          self.readBackgroundCorpusCounts(conf.bgCountFile, conf.performStemming)
        if (conf.cosine):
          if (conf.performStemming):
            print("Reading idf values" + conf.bgIdfStemmedFile)
            self.readBackgroundIdf(conf.bgIdfStemmedFile)
          else:
            print("Reading idf values" + conf.bgIdfUnstemmedFile)
            self.readBackgroundIdf(conf.bgIdfUnstemmedFile)
    # /**
    #  * Given a path to a plaintext file (summary or single document input) or multiple files (multidocument input), computes the vocabulary
    #  * distribution--unique words and and their counts overall in the file/files.
    #  */
    def computeVocabulary(self,path):
        if (not os.path.exists(path)):
          print("Cannot compute vocabulary for : non-existent file path : " + path)
        counts = {}
        if (os.path.isdir(path)):
          files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]
          for f in range(len(files)):
            with open(path+"\\"+files[f],'r') as brf:
                for fline in brf:
                  if (fline.strip()==""):
                    continue
                  fline = re.sub("[^A-Za-z0-9 ]", " ", fline)
                  toks = fline.strip().split(" ")
                  for t in range(len(toks)):
                    self.updateCounts(toks[t].lower(), counts)
            brf.close()
        else:
          bx = open(path,'r')
          ll = bx.readline()
          while (ll != ""):
            if (ll.strip()==""):
              continue
            ll = ll.replace("[^a-zA-Z0-9 ]", " ")
            tks = ll.strip().split("[ ]+")
            for tk in range(len(tks)):
              self.updateCounts(tks[tk].lower(), counts)
            ll = bx.readline()
          bx.close()
        words=[]
        freq=[]
        totalToks = 0
        em = counts.keys()
        for m in em:
          wd = str(m)
          fr = counts[wd]
          words.append(wd)
          freq.append(fr)
          totalToks += fr
        # // print(words);
        # // print(freq);
        return vocabDist(words, freq, totalToks)
    # /**
    # * Given a new token updates the frequency count of that word in the vocabulary distribution
    # */
    def updateCounts(self, word, currCounts):
        if (word in self.stopWords):
          return
        if (self.stemOption):
          word = PorterStemmer().stem(word)
        if (word in currCounts.keys()):
          cur = currCounts[word]
          currCounts.update({word: cur + 1})
        else:
          currCounts.update({word: 1})
    # /**
    # * Function to perform cosine similarity computation between two vectors
    # */
    def computeCosine(self, vecA, vecB):
        dotProd = 0.0
        vecAMag = 0.0
        vecBMag = 0.0
        cosSim = 0.0
        for n in range(len(vecA)):
            dotProd += vecA[n] * vecB[n]
            vecAMag += vecA[n] * vecA[n]
            vecBMag += vecB[n] * vecB[n]
        if (dotProd == 0.0):
            cosSim = 0.0
        else:
            cosSim = dotProd / (math.sqrt(vecAMag) * math.sqrt(vecBMag))
        return cosSim
    # /**
    # * Returns the cosine similarity given two distributions--words and frequency counts.
    # * Vector contents are tf weights.
    # */
    def computeTfCosineGivenVocabDists(self, distA, distB):
        complete = []
        complete.append(distA.vocabWords)
        for p in range(len(distB.vocabWords)):
          if (not distB.vocabWords[p] in complete):
            complete.append(distB.vocabWords[p])
        vecA = [0]*len(complete)
        vecB = [0]*len(complete)
        for i in range(len(complete)):
          indA = distA.vocabWords.index(complete[i])
          if (indA != -1):
            vecA[i] = distA.vocabFreq[indA]
          else:
            vecA[i] = 0
          indB = distB.vocabWords.index(complete[i])
          if (indB != -1):
            vecB[i] = distB.vocabFreq[indB]
          else:
            vecB[i] = 0
        return self.computeCosine(vecA, vecB)
    # /**
    # * Function to multiply tf values in given vectors with idf and compute cosine similarity
    # *
    # */
    def multiplyIdfAndGetCosine(self, stringsInVector, tfVecA, tfVecB):
        tfidf = [[0] * len(stringsInVector)] * 2
        for i in range(len(stringsInVector)):
            if (stringsInVector[i] in self.wordToIdf.keys()):
                idf = self.wordToIdf[stringsInVector[i]]
            else:
                idf = math.log(self.totalDocs)
            tfidf[0][i] = tfVecA[i] * idf
            tfidf[1][i] = tfVecB[i] * idf
        return self.computeCosine(tfidf[0], tfidf[1])
    # /**
    # * Returns the cosine similarity given two distributions of words and frequency counts
    # * Vector contents are tf*idf weights
    # */
    def computeTfIdfCosineGivenVocabDists(self, distA, distB):
        complete = []
        complete.extend(distA.vocabWords)
        for p in range(len(distB.vocabWords)):
          if (not distB.vocabWords[p] in complete):
            complete.append(distB.vocabWords[p])
        vecA = [0]*len(complete)
        vecB = [0]*len(complete)
        for i in range(len(complete)):
          try:
            indA = distA.vocabWords.index(complete[i])
            vecA[i] = distA.vocabFreq[indA]
          except:
            vecA[i] = 0
          try:
            indB = distB.vocabWords.index(complete[i])
            vecB[i] = distB.vocabFreq[indB]
          except:
            vecB[i] = 0
        # // for(int i  = 0; i < complete.size(); i++)
        # // 	{
        # // 	    print(complete.get(i)+"\t"+vecA[i]+"\t"+vecB[i]);
        # // 	}
        return self.multiplyIdfAndGetCosine(complete, vecA, vecB)
    # /**
    # * Given a set of words and frequency counts, this function computes the
    # * loglikelihood ratios in comparison with the background corpus supplied.
    # * Returns a list of loglikelihood ratios corresponding to each of
    # * the words in the given vocabulary distribution.
    # */
    def computeLogLikelihoodRatio(self, dist):
        chisqValues = []
        for i in range(len(dist.vocabWords)):
          wfreq = dist.vocabFreq[i]
          try:
            bgIndex = backgroundDist.vocabWords.index(dist.vocabWords[i])
            bgfreq = backgroundDist.vocabFreq[bgIndex]
          except:
            bgfreq = 0.0
          o11 = wfreq
          o12 = bgfreq
          o21 = dist.numTokens - wfreq
          o22 = backgroundDist.numTokens - bgfreq
          N = o11 + o12 + o21 + o22
          p = (o11 + o12) / N
          p1 = o11 / (o11 + o21)
          p2 = o12 / (o12 + o22)
          if (p == 0):
            t1 = 0.0
          else:
            t1 = math.log10(p) * (o11 + o12)
          if (p == 1):
            t2 = 0.0
          else:
            t2 = (o21 + o22) * math.log10(1 - p)
          if (p1 == 0):
            t3 = 0.0
          else:
            t3 = o11 * math.log10(p1)
          if (p1 == 1):
            t4 = 0.0
          else:
            t4 = o21 * math.log10(1 - p1)
          if (p2 == 0):
            t5 = 0.0
          else:
            t5 = o12 * math.log10(p2)
          if (p2 == 1):
            t6 = 0.0
          else:
            t6 = o22 * math.log10(1 - p2)
          loglik = -2.0 * ((t1 + t2) - (t3 + t4 + t5 + t6))
          chisqValues.append(loglik)
        return chisqValues
    # /**
    # * Given a set of words and frequency counts, this function computes the
    # * loglikelihood ratios for each word and returns those for which the
    # * ratio exceeds the given critical value.
    # * A value of 10 corresponds to a confidence level of 0.001 and is the cutoff
    # * used for our experiments.
    # */
    def getTopicSignatures(self, dist, criticalValue):
        loglikRatios = self.computeLogLikelihoodRatio(dist)
        topicWords = []
        for x in range(len(dist.vocabWords)):
          if (loglikRatios[x] > criticalValue):
            topicWords.append(dist.vocabWords[x])
        return topicWords
    # /**
    # * Clear wordToIdf mappings and stopword lists. No longer needed.
    # */
    def clearAll(self):
        self.wordToIdf.clear()
        self.stopWords.clear()
# /**
#  * Computes the macro (system-level) scores by averaging a system's score for its summaries to different input
#  * documents.
#  */
class AverageScores:
    # /**
    #  * Scores for each system are stored as lists one for each input. Calculates
    #  * average feature values across all inputs.
    #  */
    def getAverageValues(self, diffInpFeatures):
        df = "{0:.4f}".format
        avgValues = len(diffInpFeatures.getFirst())
        for inp in range(len(diffInpFeatures)):
            featValues = diffInpFeatures.get[inp]
            for feat in range(len(featValues)):
                avgValues[feat] += featValues[feat]
        numInputs = len(diffInpFeatures)
        ret = ""
        for a in range(len(avgValues)):
            ret += df(avgValues[a] / numInputs) + " "
        return ret.strip()
  # /**
  #  * Computes the system-level scores from the per input evaluation scores. 
  #  * @parameter microfile - name of micro evaluation file created from InputBasedEvaluation
  #  * @parameter startindex - index of the column in the micro file where feature values begin
  #  * @parameter endindex - the last column that is a feature value (indices start from 0)
  #  * @parameter macrofilename - name of output file
  #  */
    def computeAndWriteMacroScores(self, microfile, startindex, endindex, macrofilename):
        print("Computing average scores...")
        sysidToScores = {}
         # //first line must be header of micro file
        with open(microfile,'r') as br:
          header = br.readline()
          for miline in br:
            print(miline)
            toks = miline.strip().split(" ")
            sysid = toks[1]
            featureValues = []
            for x in range(startindex,endindex):
              featureValues.append(toks[x])
            if (sysid in sysidToScores.keys()):
              sysidToScores[sysid].append(featureValues)
            else:
              listForDiffInputs = []
              listForDiffInputs.append(featureValues)
              sysidToScores.update({sysid: listForDiffInputs})
        print("done1")
        # //all scores in memory now
        eht = sysidToScores.keys()
        maxInputs = 0
        for m in eht:
          sid = m
          inplists = sysidToScores[sid]
          if (len(inplists) > maxInputs):
            maxInputs = len(inplists)
        bw = open(macrofilename,'w')
        # //write header to macro file
        headToks = header.strip().split(" ")
        bw.write(headToks[1])
        for h in range(2,len(headToks)):
          if (h >= startindex and h <= endindex):
            bw.write(" " + "Avg" + headToks[h])
        bw.write("\n")
        print("Total number of inputs = ",maxInputs)
        eh = sysidToScores.keys()
        for m in eh:
          sid = m
          ilists = sysidToScores[sid]
          if (len(ilists) < maxInputs):
            print("Warning: Only ", len(ilists) , " summaries averaged for system " + sid + ". Max number of inputs = " + maxInputs)
          avgFeatures = self.getAverageValues(ilists)
          bw.write(sid + " " + avgFeatures)
          bw.write("\n")
        bw.close()
    # //	print("Finished computing average feature values.")
# /**
 # * Main module for evaluation. The main function takes a file containing input summary mappings and returns
 # *  evaluation scores for features specified in the config file.
 # * @author Annie Louis
 # */
# /**
#  * Stores the options specified in the config file provided.
#  *
#  * @author Annie Louis
#  */
class ConfigOptions:
  # /**
  #  * By default, stemming and stop word removal will be done. All features are computed.
  #  */
  def __init__(self):
    self.performStemming = True
    self.removeStopWords = True
    self.stopFile = ""
    self.bgCountFile = ""
    self.bgIdfUnstemmedFile = ""
    self.bgIdfStemmedFile = ""
    self.divergence = True
    self.cosine = True
    self.topic = True
    self.summProb = True
    self.topicCutoff = 10.0
class InputBasedEvaluation:
  # /**
   # * Given the vocabulary distributions of an input and its summary and a list of features to compute,
   # * the function returns a string containing the values of these features.
   # * <br><br> For smoothing KL and JS divergence the following values are assumed for bins and gamma
   # * <br> LidStone smoothing: The total possible outcomes ie, words in the vocabulary distribution
   # * are set as 1.5 times the input vocabulary size. The fractional count added to the original count
   # * for each vocabulary item is 0.005.
   # */
  def generateFeatures(self,inputDist, summaryDist, listOfFeatures, feat, cbu, topicCutoff):
    # // for smoothing kl and js divergence the following values are assumed for bins and gamma
    # // LidStone smoothing:
    # // the total possible outcomes ie, words in the vocabulary distribution are set as 1.5 times the input vocabulary size
    # // the fractional count added to the original count for each vocabulary item is 0.005
    myFormatter = "{0:.4f}".format
    featString = ""
    if ("KLInputSummary" in listOfFeatures):
      klarray = feat.getKLdivergenceSmoothed(inputDist, summaryDist, 0.005, 1.5 * len(inputDist.vocabWords))
      featString += myFormatter(klarray[0]) + " " + myFormatter(klarray[1]) + " "
      featString += myFormatter(feat.getJSDivergence(inputDist, summaryDist)) + " "
      featString += myFormatter(feat.getSmoothedJSDivergence(inputDist, summaryDist, 0.005, 1.5 * len(inputDist.vocabWords))) + " "
    if ("cosineAllWords" in listOfFeatures):
      featString += myFormatter(cbu.computeTfIdfCosineGivenVocabDists(inputDist, summaryDist)) + " "
    if ("percentTopicTokens" in listOfFeatures):
      inputTopicWords = cbu.getTopicSignatures(inputDist, topicCutoff)
      percentTokensTopicWords = myFormatter(feat.getPercentTokensThatIsSignTerms(inputTopicWords, summaryDist))
      fractionTopicWordsCovered = myFormatter(feat.getPercentTopicWordsCoveredByGivenDist(inputTopicWords, summaryDist))
      featString += percentTokensTopicWords + " " + fractionTopicWordsCovered + " "
      topicWordFrequencies = []
      totalCount = 0
      for tp in range(len(inputTopicWords)):
        indtopic = inputDist.vocabWords.index(inputTopicWords[tp])
        freq = inputDist.vocabFreq[indtopic]
        topicWordFrequencies.append(freq)
        totalCount += freq
      inputTopicDist = vocabDist(inputTopicWords, topicWordFrequencies, totalCount)
      topicOverlap = myFormatter(cbu.computeTfIdfCosineGivenVocabDists(inputTopicDist, summaryDist))
      featString += topicOverlap + " "
    if ("unigramProb" in listOfFeatures):
      uniprob = feat.getUnigramProbability(inputDist, summaryDist)
      multprob = feat.getMultinomialProbability(inputDist, summaryDist)
      featString += myFormatter(uniprob) + " " + myFormatter(multprob) + " "
    return featString.strip()
  # /**
   # * Reads configuration options as provided in config file argument to main function
   # */
  def readAndStoreConfigOptions(self, configFile):
    cf = ConfigOptions()
    bcr = open(configFile,'r')
    cline = bcr.readline()
    while (cline != ""):
      cline = cline.strip()
      if (cline==""):
        continue
      if (cline.startswith("-")):
        continue
      if (cline.startswith("=")):
        continue
      cline = cline.replace(" ", "")
      clineToks = cline.split("=")
      cline = bcr.readline()
      if clineToks[0]=="performStemming":
        if clineToks[1].lower()=="n":
          cf.performStemming = False
      if clineToks[0]=="removeStopWords":
        if clineToks[1].lower()=="n":
          cf.removeStopWords = False
      if (clineToks[0]=="divergence"):
        if (clineToks[1].lower()=="n"):
          cf.divergence = False
      if (clineToks[0]=="cosineOverlap"):
        if (clineToks[1].lower()=="n"):
          cf.cosine = False
      if (clineToks[0]=="topicWordFeatures"):
        if (clineToks[1].lower()=="n"):
          cf.topic = False
      if (clineToks[0]=="frequencyFeatures"):
        if (clineToks[1].lower()=="n"):
          cf.summProb = False
      if (clineToks[0]=="stopFilePath"):
        cf.stopFile = clineToks[1]
      if (clineToks[0]=="backgroundCorpusFreqCounts"):
        cf.bgCountFile = clineToks[1]
      if (clineToks[0]=="backgroundIdfUnstemmed"):
        cf.bgIdfUnstemmedFile = clineToks[1]
      if (clineToks[0]=="backgroundIdfStemmed"):
        cf.bgIdfStemmedFile = clineToks[1]
      if (clineToks[0]=="topicWordCutoff"):
        cf.topicCutoff = clineToks[1]
    if (cf.removeStopWords):
      if (cf.stopFile==""):
        print("Error in config file: must specify file with stopwords for removeStopWords=Y option")
      else:
        pass
    if (cf.cosine):
      if (cf.bgIdfUnstemmedFile=="" and not cf.performStemming):
        print("Error in config file: must specify file with idf file (unstemmed) to compute cosine overlap")
      if (cf.bgIdfStemmedFile=="" and cf.performStemming):
        print("Error in config file: must specify file with idf file (stemmed) to compute cosine overlap")
    if (cf.topic):
      #//print(cf.bgFile)
      if (cf.bgCountFile==""):
        print("Error in config file: must specify file with background corpus counts to compute topic word based features")
      if (cf.topicCutoff <= 0):
        print("Topic word cutoff must be greater than zero to be meaningful, default value is 10.0")
    # // if(!(cf.cosine || cf.topic))
    # //     {
    # // 	cf.bgFile = "";
    # // 	cf.topicCutoff = 0.0;
    # //     }
    return cf
  # /**
   # * Computes input and system level scores according to desired features. Takes two command line arguments.
   # * <br><br> 1) path to mappings file where each line is of the following format
   # * <br> inputid &lt;space&gt; systemid &lt;space&gt; path_to_input_file/files &lt;space&gt; path_to_summary_file
   # * <br> 2) path to a file containing configuration settings
   # * <br><br> Creates two files in the same directory as mappings file - mappingsfile.ieval.macro (system level scores--average over test set)
   # * and mappingsfile.ieval.micro (input level scores--for each individual input)
   # */
ieval = InputBasedEvaluation()
mappingsFile = r"sampleMappings.txt"# input("Mapping File Path")
configFile = r"config.example" # input("Config File Path")
stopFile = ""
bgFile = ""
idfFile = ""
opt = ieval.readAndStoreConfigOptions(configFile)
featuresToCompute = []
if (opt.divergence):
    featuresToCompute.append("KLInputSummary")
    featuresToCompute.append("KLSummaryInput")
    featuresToCompute.append("unsmoothedJSD")
    featuresToCompute.append("smoothedJSD")
if (opt.cosine):
    featuresToCompute.append("cosineAllWords")
if (opt.topic):
    featuresToCompute.append("percentTopicTokens")
    featuresToCompute.append("fractionTopicWords")
    featuresToCompute.append("topicWordOverlap")
if (opt.summProb):
    featuresToCompute.append("unigramProb")
    featuresToCompute.append("multinomialProb")
# cbu = CorpusBasedUtilities(opt)
# feat = EvalFeatures()
# bwmicro = open(mappingsFile + ".ieval.micro",'w')
# for f in range(len(featuresToCompute)):
#     print(featuresToCompute[f])
#     bwmicro.write(" " + featuresToCompute[f])
#     bwmicro.write("\n")
#     bmr = open(mappingsFile,'r')
#     line = bmr.readline()
#     while (line != ""):
#         toks = line.strip().split(" ")
#         if (len(toks) != 4):
#             print("Error in mappings file: " + mappingsFile
#             + "\n must have the format inputid <space> systemid <space> path_to_input_vocabulary_file <space> path_to_summary_vocabulary_file")
#         bwmicro.write(toks[0] + " " + toks[1] + " ")
#         print("Evaluating summary " + toks[1] + " for input " + toks[0])
#         inputVocabDist = cbu.computeVocabulary(toks[2])
#         summaryVocabDist = cbu.computeVocabulary(toks[3])
#         features = ieval.generateFeatures(inputVocabDist, summaryVocabDist, featuresToCompute, feat, cbu, opt.topicCutoff)
#         print("D")
#         bwmicro.write(features)
#         bwmicro.write("\n")
#         line = bmr.readline()
# bwmicro.close()
    #//other tables no longer needed
#//compute average feature values - sys level evaluation
numFeatures = len(featuresToCompute)
AverageScores().computeAndWriteMacroScores(mappingsFile + ".ieval.micro", 2, numFeatures + 1, mappingsFile + ".ieval.macro")
print("done")
